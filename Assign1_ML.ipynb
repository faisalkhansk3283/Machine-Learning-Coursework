{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfkfXLzD-Y64",
        "outputId": "bb7bb19a-46bd-4537-8f19-0e5109e47cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.2974)\n",
            "tensor(3.2974)\n",
            "tensor(3., grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#1.e\n",
        "import torch\n",
        "\n",
        "def fsum(theta,x,k):\n",
        "  t=0\n",
        "  p=x-theta\n",
        "  #for i in range(1,k+1):\n",
        "  t=t+torch.exp(k*(p**k))\n",
        "\n",
        "  #print(t)\n",
        "  t.backward()\n",
        "    \n",
        "  return t#torch.log(t)\n",
        "\n",
        "def fmul(theta,x,k):\n",
        "  s=1\n",
        "  #for i in range(1,k+1):\n",
        "    \n",
        "  s=s*torch.exp(k*(x-theta)**k)\n",
        "  #print(s,i*((x-theta)**i))\n",
        "  #print(s)\n",
        "  s.backward()\n",
        "  \n",
        "  return s#torch.log(t)\n",
        "\n",
        "k=torch.tensor(2)\n",
        "x=torch.tensor(1.5,requires_grad=True )\n",
        "theta=torch.tensor(1 )\n",
        "y=torch.tensor(0)\n",
        "z=torch.tensor(0)\n",
        "y=fsum(theta,x,k)\n",
        "#y.backward(retain_graph=True)\n",
        "#print(y)\n",
        "y=torch.log(y)\n",
        "#y.backward()\n",
        "print(x.grad)\n",
        "xx=torch.tensor(1.5,requires_grad=True )\n",
        "z=fmul(theta,xx,k)\n",
        "z=torch.log(z)\n",
        "print(x.grad)\n",
        "\n",
        "\n",
        "\n",
        "derivative_result=0\n",
        "\n",
        "\n",
        "for i in range(1,k+1):\n",
        "  derivative_result+=torch.mul(i**2,(x-theta)**(i-1))\n",
        "\n",
        "print(derivative_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.h\n",
        "\n",
        "import torch\n",
        "\n",
        "#Given Matrix A\n",
        "A=torch.tensor([[5.,10.],[-2.,0.],[1.,-1.]])\n",
        "\n",
        "#Matrix A Transpose(AT)\n",
        "ATran=torch.transpose(A,0,1)\n",
        "print('Matrix A:\\n',A,'\\nMatrix A Transpose:\\n',ATran)\n",
        "\n",
        "#A*AT & AT*A\n",
        "Res1_Matrix=torch.matmul(A,ATran)\n",
        "Res2_Matrix=torch.matmul(ATran,A)\n",
        "print('A * AT:\\n',Res1_Matrix,'\\nAT * A:\\n',Res2_Matrix)\n",
        "\n",
        "#Determinant of A*AT and AT*A\n",
        "Det1=(Res1_Matrix[0][0] * (Res1_Matrix[1][1]*Res1_Matrix[2][2] - Res1_Matrix[2][1]*Res1_Matrix[1][2])\n",
        "    -Res1_Matrix[1][0] * (Res1_Matrix[0][1]*Res1_Matrix[2][2] - Res1_Matrix[2][1]*Res1_Matrix[0][2])\n",
        "    +Res1_Matrix[2][0] * (Res1_Matrix[0][1]*Res1_Matrix[1][2] - Res1_Matrix[1][1]*Res1_Matrix[0][2]))#.type(torch.FloatTensor))\n",
        "Det2=torch.det(Res2_Matrix)#.type(torch.FloatTensor))\n",
        "print('\\nDet1:',Det1,'\\nDet2:',Det2)\n",
        "\n",
        "#Trace of A*AT and AT*A\n",
        "Trace_Res1=sum(torch.diag(Res1_Matrix,diagonal=0))\n",
        "Trace_Res2=sum(torch.diag(Res2_Matrix,diagonal=0))\n",
        "print('\\nTrace (A AT):',Trace_Res1,'\\nTrace (AT A):',Trace_Res1)\n",
        "\n",
        "#Forbenius Norm\n",
        "print('\\ninbuilt Method frobenius Square:',torch.frobenius_norm(A)**2)\n",
        "x,y=A.shape\n",
        "frob1_Square=0\n",
        "for i in range(x):\n",
        "  for j in range(y):\n",
        "    frob1_Square+=A[i][j]*A[i][j]\n",
        "\n",
        "print('frobenius_norm_Square:',frob1_Square)\n",
        "print('\\nfrobenius_norm is equal to square root of Trace(A AT)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKDXSxBSnZ9R",
        "outputId": "4dabe41d-ff21-41c8-d4bf-0a6c88298bd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " tensor([[ 5., 10.],\n",
            "        [-2.,  0.],\n",
            "        [ 1., -1.]]) \n",
            "Matrix A Transpose:\n",
            " tensor([[ 5., -2.,  1.],\n",
            "        [10.,  0., -1.]])\n",
            "A * AT:\n",
            " tensor([[125., -10.,  -5.],\n",
            "        [-10.,   4.,  -2.],\n",
            "        [ -5.,  -2.,   2.]]) \n",
            "AT * A:\n",
            " tensor([[ 30.,  49.],\n",
            "        [ 49., 101.]])\n",
            "\n",
            "Det1: tensor(0.) \n",
            "Det2: tensor(629.)\n",
            "\n",
            "Trace (A AT): tensor(131.) \n",
            "Trace (AT A): tensor(131.)\n",
            "\n",
            "inbuilt Method frobenius Square: tensor(131.)\n",
            "frobenius_norm_Square: tensor(131.)\n",
            "\n",
            "frobenius_norm is equal to square root of Trace(A AT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.b\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "### This class is NOT a Pytorch layer. You'll need to turn it into one.\n",
        "class Eqn1(nn.Module):\n",
        "  \n",
        "  def __init__(self, dim=2, omega=None):\n",
        "    \"\"\"\n",
        "    * omega is a vector of size 2\n",
        "    \"\"\"\n",
        "    super(Eqn1, self).__init__()\n",
        "    ### some initialization is missing....\n",
        "    if omega is not None:\n",
        "      self.omega = omega\n",
        "    else:\n",
        "      self.omega = torch.zeros(size=(dim,))\n",
        "\n",
        "  ### continue implementing this class, in order to turn it into a \n",
        "  ### Pytorch layer\n",
        "  def forward(self, Ys):\n",
        "    \"\"\"\n",
        "    Ys is a binary vector of size N.\n",
        "\n",
        "    This function computes Eqn (1) on the provides `Ys`, according to the\n",
        "    current `omega` of the model.\n",
        "    \"\"\"\n",
        "    compute_value=0\n",
        "    for i in range(len(Ys)):\n",
        "     compute_value=compute_value+(-1)*omega[Ys[i]]+torch.log(torch.exp(omega[0])+torch.exp(omega[1]))\n",
        "    #compute_value=compute_value+(-1)*omega[Ys[N]]+torch.log(torch.exp(omega[0])+torch.exp(omega[1]))\n",
        "    print(compute_value)\n",
        "    return compute_value\n",
        "    #pass\n",
        "\n",
        "N=5\n",
        "Ys=torch.tensor([1,0,0,1,1])\n",
        "omega=torch.tensor([0.5,-0.2] )\n",
        "pyLayer=Eqn1(dim=2,omega=omega)\n",
        "pyout=pyLayer(Ys)\n",
        "#pyLayer=Eqn1(torch.tensor([1,0,0,1,1]))\n",
        "#pyLayer=Eqn1(dim=2,omega=torch.tensor([0.5,-0.2] ))\n",
        "#pyLayer.forward(torch.tensor([1,0,0,1,1]))\n",
        "\n",
        "\n",
        "N=8\n",
        "Ys=torch.tensor([1,0,0,1,1,0,1,1])\n",
        "omega=torch.tensor([0.5,-0.5] )\n",
        "pyLayer=Eqn1(dim=2,omega=omega)\n",
        "pyout=pyLayer(Ys)\n",
        "#pyLayer=Eqn1(omega=torch.tensor([0.5,-0.5]))\n",
        "#pyLayer.forward([1,0,0,1,1,0,1,1])\n",
        "\n",
        "N=int(input(\"Enter N\"))\n",
        "Ys=torch.tensor(list(map(int,input(\"Enter n elements\").split()[:N])))\n",
        "omega=torch.tensor([0.5,-0.5] )\n",
        "pyLayer=Eqn1(dim=2,omega=omega)\n",
        "pyout=pyLayer(Ys)\n",
        "print(Ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_WXjeXKkKsG",
        "outputId": "45245b44-39c4-4da5-b1f1-36ab43273835"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.1159)\n",
            "tensor(7.5061)\n",
            "Enter N5\n",
            "Enter n elements0 1 0 1 1\n",
            "tensor(4.5663)\n",
            "tensor([0, 1, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.c.ii\n",
        "import torch\n",
        "\n",
        "def fw0(w,y,n):\n",
        "  t=0\n",
        "  s=0\n",
        "  #t=t+(-w[y[n]])+torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  t=t+((n-2)*-w1)+(n-2)*torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  s=s+((n-3)*-w0)+(n-3)*torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  l=s+t\n",
        "  #t.backward()\n",
        "  l.backward()\n",
        "    \n",
        "  return l\n",
        "\n",
        "n=torch.tensor(5)\n",
        "#w=torch.tensor([0.5,-0.2] )\n",
        "w0=torch.tensor(0.5,requires_grad=True )\n",
        "w1=torch.tensor(-0.2,requires_grad=True )\n",
        "w=torch.tensor([w0,w1] )\n",
        "y=torch.tensor([300,1,0,0,1,1])\n",
        "#z=torch.tensor(0)\n",
        "z=fw0(w,y,n)\n",
        "\n",
        "print(w0.grad)\n",
        "print(w1.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Y4ZrlDP06t",
        "outputId": "c8b26d3c-5365-47dc-b532-741c30289404"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.3409)\n",
            "tensor(-1.3409)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.D\n",
        "import torch\n",
        "\n",
        "def fw0(w,y,n):\n",
        "  t=0\n",
        "  s=0\n",
        "  #t=t+(-w[y[n]])+torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  t=t+((n-2)*-w1)+(n-2)*torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  s=s+((n-3)*-w0)+(n-3)*torch.log(torch.exp(w0)+torch.exp(w1))\n",
        "  l=s+t\n",
        "  #t.backward()\n",
        "  l.backward()\n",
        "    \n",
        "  return l\n",
        "\n",
        "\n",
        "n=torch.tensor(5)\n",
        "#eq=((n-3)*-w0)+(n)*torch.log(torch.exp(w0)+torch.exp(w1))+((n-2)*-w1)\n",
        "\n",
        "#w=torch.tensor([0.5,-0.2] )\n",
        "w0=torch.tensor(0.0,requires_grad=True )\n",
        "w1=torch.tensor(0.0,requires_grad=True )\n",
        "w=torch.tensor([w0,w1] )\n",
        "y=torch.tensor([300,1,0,0,1,1])\n",
        "#z=torch.tensor(0)\n",
        "z=fw0(w,y,n)\n",
        "print(w0.grad)\n",
        "print(w1.grad)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Fzcb2Yc6pQ",
        "outputId": "fc6a4739-244a-4f0b-f8e4-f99c044cd160"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5000)\n",
            "tensor(-0.5000)\n"
          ]
        }
      ]
    }
  ]
}